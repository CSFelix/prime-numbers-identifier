{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb8757f-576c-406a-b5a9-656f2f1b4f5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h2 align=\"center\">üî¢ Prime Numbers Identifier</h2>\n",
    "<br />\n",
    "\n",
    "<i><center>Let's dive into this interesting world</center></i>\n",
    "\n",
    "<br />\n",
    "\n",
    "üëã Hello guys, today we will go into an advanced math topic: **Prime Numbers**. Our task here is to understand what they are, their importance and usages in real-life, their characteristics and create Python Codes to figure out whether a number is prime or not using *Riemann Hypothesis* and *Machine/Deep Learning Algorithms*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656f6a2-29f1-4120-9090-4c0f5e2013e4",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id='problem-description'>üìù Problem Description</h2>\n",
    "\n",
    "<b>- Characteristics</b>\n",
    "\n",
    "`Prime Numbers` are especific natural numbers that cannot be divised by other natural ones, but itself and 1. Natural numbers are those ones that are positives and integers, that is, fall into the range from 1 to positive infinite. And you may be wondering: \"*Yeah, yeah, I learned about them on school. So what? What do they have that make them so special?*\". Well, let's go to the explanation!\n",
    "\n",
    "You probably have ever heard from someone or read on the internet that you can do anything with your computer, especially if you know how to program. I am afraid, but I have bad news: That's *\"bullshit\"*! To a problem be considered possible to be solved by computers, it has to follow two conditions: 1) a pattern to solve the problem must exist and be possible to be implemented with a computer; and 2) the implementation should solve it in a considerable period of time using a considerable process cost. \n",
    "\n",
    "In Computer Science there are the **Nondeterministic Polynomial Problems (*NPs*)**, they are problems that does not follow one or both of the previous conditions, and guess what? Identifying `whether a number is prime or not` is one of them - just for curiosity, all NP problems are listed here: [List of Unsolved Problems in Mathematics](https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics).\n",
    "\n",
    "Guessing whether 5, 8, 80 and 987 are primes or not is a easy task for a computer, but when the number of digits increases, it becomes a tough task, demanding more time and process cost to solve it. For instance, consider the number 983,129,319,128,349,245,423,213,643,369,126,335,654,234,126,324,363, quite a big one, right? This one would take years to be identified as prime or non-prime by a computer.\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://interestingengineering.com/_next/image?url=https%3A%2F%2Fimages.interestingengineering.com%2F1200x675%2Ffilters%3Aformat(webp)%2Fimages%2FNOVEMBER%2Fprime_numbers.jpg&w=1920&q=75\" alt=\"An image containing random numbers\" />\n",
    "    <figcaption>Fig. 1 - Numbers. <a href=\"https://interestingengineering.com/science/the-incredible-importance-of-prime-numbers-in-daily-life\" target=\"_blank\"><b>Unacademy</b><sup>¬©</sup></a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Cyber Security</b>\n",
    "\n",
    "Having this in mind, Security Engineers chosen the Prime Numbers to be the base of Cyber Security Cyphers, such as `Hash Tables` and `Rivest‚ÄìShamir‚ÄìAdleman (RSA) Algorithm`. In `RSA`, two large prime numbers are multiplied to result in a huge number to generate the `public and private keys` that will be used to encrypt and decrypt messages respectively. If a computer be able to decompose this huge number, the machine would be able to get the prime numbers that generated it and then, calculate the `private key` and decript any message encrypted by its public key.\n",
    "\n",
    "If this becomes possible, the cyber-security in general would be in chaos. Imagine hackers getting access to your private messages and bank accounts. Now think bigger, picture hackers getting access to government databases and stealing and encrypting critical datas. The world would be in collapse!!\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Riemann's Hypothesis</b>\n",
    "\n",
    "`Riemann's Hypothesis` is a Hypothesis Equation that tells if the the number is prime or not. To our better understanding, consider the spiral below:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://www.naturalnumbers.org/archspiralrnd.png\" alt=\"Archimedean Spiral\" />\n",
    "    <figcaption>Fig. 2 - Archimedean Spiral. <a href=\"http://www.naturalnumbers.org/sparticle.html\" target=\"_blank\"><b>Michael M. Ross at naturalnumbers.org</b>¬©</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "Now, let's fill the spiral line with numbers from 0 to infinite positive starting at the center.\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://www.naturalnumbers.org/numberline.png\" alt=\"Archimedean Spiral with numbers\" />\n",
    "    <figcaption>Fig. 3 - Archimedean Spiral with numbers. <a href=\"http://www.naturalnumbers.org/sparticle.html\" target=\"_blank\"><b>Michael M. Ross at naturalnumbers.org</b>¬©</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "Identifying the `counterclockwise`, that is, the positions where a [perfect square](https://en.wikipedia.org/wiki/Golden_spiral \"Golden spiral\") is formed, we'll realize that these positions are always in the east side - `1, 4, 9...`:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://www.naturalnumbers.org/spiraldirection.png\" alt=\"Archimedean Spiral counterclockwises\" />\n",
    "    <figcaption>Fig. 4 - Archimedean Spiral counterclockwises. <a href=\"http://www.naturalnumbers.org/sparticle.html\" target=\"_blank\"><b>Michael M. Ross at naturalnumbers.org</b>¬©</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "And, after calculating the `Spiral's Product Curves`, we'll notice that all results corresponds to **Prime Numbers Positions (*Darkest Dots*)**:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://www.naturalnumbers.org/numberdots.png\" alt=\"Archimedean Spiral's Product Curves\" />\n",
    "    <figcaption>Fig. 5 - Archimedean Spiral's Product Curves. <a href=\"http://www.naturalnumbers.org/sparticle.html\" target=\"_blank\"><b>Michael M. Ross at naturalnumbers.org</b>¬©</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "As more products we calculate, as more prime numbers we will find; and tracing a sequential line on each dot position, we'll get another perfect spiral!\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://www.naturalnumbers.org/p+41.png\" alt=\"Archimedean Spiral's Product Curves Lined\" />\n",
    "    <figcaption>Fig. 6 - Archimedean Spiral's Product Curves Lined. <a href=\"http://www.naturalnumbers.org/sparticle.html\" target=\"_blank\"><b>Michael M. Ross at naturalnumbers.org</b>¬©</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "Taking this into account, Riemann Hypothesis gives an equation to calculate if the dot position of a given number fits a prime number dot in an *Archimedean Spiral*.\n",
    "\n",
    "```python\n",
    "zeta(s) = sum(1/n**s) # being n from 1 to positive infinite\n",
    "```\n",
    "\n",
    "$$\n",
    "\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s}\n",
    "$$\n",
    "\n",
    "Even though this equation is the closest one to call as a pattern to identify prime numbers and working out for the first thousand ones, we cannot say that it is 100% correct since we cannot test it out to all existent numbers!\n",
    "\n",
    "**OBS.:** this was a short explanation about Riemann Hypothesis, if you wanna see it deeper, I would recommend watching the [The Riemann Hypothesis, Explained](https://www.youtube.com/watch?v=zlm1aajH6gY) YouTube video from *Quanta Magazine* channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ba568-ee8b-4d95-a8a7-7fba910b4151",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"files-description\">üìÅ Files Description</h2>\n",
    "\n",
    "> **first_million_prime_numbers.csv and first_million_prime_numbers.parquet** - files containing the first million prime numbers.\n",
    "\n",
    "> **natural_numbers.csv and natural_numbers.parquet** - files containing the numbers from one to the first million prime number.\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "The `.parquet` files are more suited for large datasets. In order to don't take a big section explaining the differences between CSV and Parquet files, here is a cheat sheet for you:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:828/1*lPfVDKtBBsZddmYP6fWeeA.gif\" alt=\"Comparison between CSV, Parquet and JSON Files\" />\n",
    "    <figcaption>Fig. 7 - Comparison between CSV, Parquet and JSON Files. Even though Parquet files are harder to be read, they are more suitable for larger and complexes datasets. Also, you can read this type of files using any Apache tools, such as `Spark`, `Hadoop` and `Hive`. <a href=https://weber-stephen.medium.com/csv-vs-parquet-vs-json-for-data-science-cf3733175176\"\" target=\"_blank\"><b>Weber Stephen at Medium</b><sup>¬©</sup></a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deef208-db57-412a-854b-fcea7172820e",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"features\">‚ùì Features</h2>\n",
    "\n",
    "> **Rank** - prime number index;\n",
    "\n",
    "> **Num** - number;\n",
    "\n",
    "> **Interval** - interval between the current and the previous prime number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70567393-cc07-488d-b87c-c9f87bbc6a0d",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"target\">üåü Target</h2>\n",
    "\n",
    "> **Is Prime** - tells whether a number is prime (*True*) or not (*False*).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Is Prime</th>\n",
    "        <th>Meaning</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>True</td>\n",
    "        <td>Number is prime</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>False</td>\n",
    "        <td>Number is not prime</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8894d02-a2ee-4147-81a2-37249eee1761",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"metric\">üìè Metric</h2>\n",
    "\n",
    "About the metrics, we will apply different ones to each solution.\n",
    "\n",
    "For `Riemann Hypothesis` and the `Machine Learning Algorithm`, we will use `Classification Accuracy` or `Balanced Accuracy`; whereas for the `Deep Learning Model` we will use `Binary Accuracy` with `Binary Cross-Entropy` Loss Function. Let's see an explanation for each one.\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Classification Accuracy</b>\n",
    "\n",
    "This metric takes four types of results to evaluate the model:\n",
    "\n",
    "<br />\n",
    "\n",
    "> **True Positive (TP)** - the model predicted true and the real outcome is true; ‚úîÔ∏è\n",
    "\n",
    "> **True Negative (TN)** - the model predicted false and the real outcome is false; ‚úîÔ∏è\n",
    "\n",
    "> **False Positive (FP)** - the predicted true and the real outcome is false; ‚ùå\n",
    "\n",
    "> **False Negative (FN)** - the model predicted false and the real outcome is true. ‚ùå\n",
    "\n",
    "<br />\n",
    "\n",
    "With this in mind, Classification Accuracy takes the sum of `TP` and `TN` and divides by the sum between the four types of outcomes. The result will be the evaluation value.\n",
    "\n",
    "```python\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{{\\text{TP} + \\text{TN}}}{{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}}\n",
    "$$\n",
    "\n",
    "<br />\n",
    "Even though a good accuracy score varies for each problem case, 95% of accuracy is quite perfect to validate models for the majority of problems!!\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Balanced Accuracy</b>\n",
    "\n",
    "`Balanced Accuracy` is similar to `Accuracy` but it's used in Classification Problems when the target feature values are not balanced, that is, there are more values of class 1 rather than class 2. In these cases, Accuracy does not tell the true reality of our model's accuracy, then Balanced Accuracy comes in action!!\n",
    "\n",
    "This metric is calculated by multiplying 1/2 by the sum of `Correct Positive Predictions (True Positives - TP)` divided by the `Real Number of Positive Outcomes (Real Positives - RP)` and the `Correct Negative Predictiions (True Negatives - TN)` divided by the `Real Number of Negative Outcomes (Real Negatives - RN)`.\n",
    "\n",
    "```python\n",
    "balanced_accuracy = (1 / 2) * ((TP / RP) + (TN / RN))\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{Balanced Accuracy} = \\frac{1}{2} \\left( \\frac{TP}{RP} + \\frac{TN}{RN} \\right)\n",
    "$$\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Binary Accuracy and Binary Cross-Entropy</b>\n",
    "\n",
    "Since Deep Learning Models uses *Stocastic Descendent Gradients (SGDs)* to adjust the weights and bias, it needs a *Loss Function* that changes smoothly, however, as far as *Classification Accuracy* is a ratio of counts, it changes in jumps - becoming not suited for Deep Learning Evaluation and this is when `Binary Accuracy and Cross-Entropy` comes in action!!\n",
    "\n",
    "The figure below shows the changes of the model's losses for each possible accuracy. Realize that the changes are smoothly and not in jumps and, as higher the accuracy as lower the loss.\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://storage.googleapis.com/kaggle-media/learn/images/DwVV9bR.png\" alt=\"Line Plot - Loss x Predicted Probability of Correct Class (Accuracy)\" />\n",
    "    <figcaption>Fig. 8 - Line Plot of Loss by Predicted Probability of Correct Class (Accuracy). <a href=\"https://www.kaggle.com/code/ryanholbrook/binary-classification\" target=\"_blank\"><b>Kaggle</b><sup>¬©</sup></a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "Its equation is given as below:\n",
    "\n",
    "```python\n",
    "Hp(q) = -1 * (1/n) * sum(y * log(p(y)) + (1 - y) * log(1 - p(y)))\n",
    "```\n",
    "\n",
    "$$\n",
    "H_p(q) = -\\frac{1}{n} \\times \\sum_{i=1}^{n} \\left( y_i \\times \\log(p(y_i)) + (1 - y_i) \\times \\log(1 - p(y_i)) \\right)\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Where:</b>\n",
    "    <br /><br />\n",
    "    <p>\n",
    "        - n >> number of outcomes;<br />\n",
    "        - y >> predicted outcome (1 for True and 0 for False);<br />\n",
    "        - log(p(y)) >> log probability for y = 1 (True);<br />\n",
    "        - log(1 - p(y)) >> log probability for y = 0 (False).\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd0dd5-cc05-447c-8f44-22fefd1286b2",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"limitations\">üõë Limitations</h2>\n",
    "\n",
    "As far as the dataset just contains the firs million prime numbers, we cannot guarantee that the model will be able to identify all possible prime numbers correctly, since our dataset does not contain all existent ones listed!!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Riemann Hypothesis:</b> Realize that the limitation we got here is the same one present in Riemann Hypothesis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d776d-78d4-426a-885d-ed6be9b44e68",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"goals\">üéØ Goals</h2>\n",
    "\n",
    "> **Goal 1** - create `natural_numbers.csv` and `natural_numbers.parquet` files to store all numbers from 1 to the first million prime;\n",
    "\n",
    "> **Goal 2** - implement `Riemann Hypothesis` without Machine Learning Algorithm;\n",
    "\n",
    "> **Goal 3** - implement `Machine Learning Algorithms` to classify a given number into prime and non-prime;\n",
    "\n",
    "> **Goal 4** - implement a `Deep Learning Algorithm` to classify a given number into prime and non-prime;\n",
    "\n",
    "> **Goal 5** - do the `Machine/Deep Learning Explainability` with the best model;\n",
    "\n",
    "> **Goal 6** - export the best Machine Learning Model and the Deep Learning Model into `Pickles File Format`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9b607-5095-49ab-84d5-55428c98f908",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"setup\">‚öôÔ∏è Setup</h2>\n",
    "\n",
    "> Tools\n",
    "\n",
    "```\n",
    "- Python 3.10.x version or later;\n",
    "- Jupyter Lab;\n",
    "- Jupyter Notebook.\n",
    "```\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "> Libraries\n",
    "\n",
    "```\n",
    "- Pandas, Numpy, Matplotlib, Seaborn, Sympy and Scipy;\n",
    "- Pandas Profiling and Lux API;\n",
    "- Apache Spark and Java JDK version 8 or later (if you wanna use Apache Spark rather Pandas);\n",
    "- Pyarrow (to generate .parquet files);\n",
    "- Scikit Learn, Tensor Flow and Keras.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038c815-24ae-40e1-9c55-279b0fe62091",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"aknowledgements\">üéâ Acknowledgements</h2>\n",
    "\n",
    "> `first_million_prime_numbers` dataset provided by <a href=\"http://www.naturalnumbers.org/primes.html\" target=\"_blank\"><b>Michael M. Ross</b> in <i>naturalnumbers.org</i></a>.\n",
    "\n",
    "> `The Riemann Hypothesis, Explained` YouTube video provided by <a href=\"https://www.youtube.com/watch?v=zlm1aajH6gY\" target=\"_blank\"><b>Quanta Magazine</b> channel</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0d98e-d8be-46b2-b723-4cb74831093e",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"environment\">üß∞ Environment</h2>\n",
    "\n",
    "To setup `Lux API`, follow the steps below:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p>- Installing on Jupyter Notebook and Jupyter Lab: <kbd>pip install lux-api</kbd></p>\n",
    "    <p>- Activating Extension for Jupyter Notebook: <kbd>jupyter nbextension install --py luxwidget</kbd> and <kbd>jupyter nbextension enable --py luxwidget</kbd></p>\n",
    "    <p>- Activating Extension for Jupyter Lab: <kbd>jupyter labextension install @jupyter-widgets/jupyterlab-manager</kbd> and <kbd>jupyter labextension install luxwidget</kbd></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604bc8cb-5b13-4d3a-a3c4-f081d003e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Libraries ----\n",
    "import pandas as pd # pip install pandas\n",
    "import sympy # pip install sympy\n",
    "import scipy.stats as stats # pip install scipy\n",
    "import numpy as np # pip install numpy\n",
    "import matplotlib.pyplot as plt # pip install matplotlib\n",
    "import seaborn as sns # pip install seaborn\n",
    "import pyarrow # pip install pyarrow\n",
    "\n",
    "# import lux # pip install lux-api >> this one will probably returns MemoryError, if that's the case, you can let this line commented\n",
    "# import pandas_profiling # pip install pandas-profiling >> deprecated\n",
    "import ydata_profiling # pip install pandas-profiling\n",
    "\n",
    "# ---- Constants ----\n",
    "SEED = (5466)\n",
    "PRIME_NUMBERS_FILE_PATH_CSV = (\"./dataset/first_million_prime_numbers.csv\")\n",
    "PRIME_NUMBERS_FILE_PATH_PARQUET = (\"./dataset/first_million_prime_numbers.parquet\")\n",
    "NATURAL_NUMBERS_FILE_PATH_CSV = (\"./dataset/natural_numbers.csv\")\n",
    "NATURAL_NUMBERS_FILE_PATH_PARQUET = (\"./dataset/natural_numbers.parquet\")\n",
    "\n",
    "\n",
    "# ---- Seeds ----\n",
    "np.random.seed(seed = SEED) # scipy uses the same generator as numpy, so setting seed on numpy will automatically set on scipy too!\n",
    "\n",
    "# ---- Others ----\n",
    "sns.set_style('whitegrid')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large', titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed7052-af21-47d6-98c8-3146dc785753",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"descriptive-analysis\">üîç Descriptive Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94592f7f-dedc-4204-ae1d-fcf99c9a5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Reading Dataset and Overviewing It ----\n",
    "df = pd.read_csv(PRIME_NUMBERS_FILE_PATH_CSV, index_col=['Rank'])\n",
    "\n",
    "print(f'# of Observations: {df.shape[0]}')\n",
    "print(f'# of Features: {df.shape[1]} ({df.columns})')\n",
    "print('----')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07eff93-fb69-4776-b683-a4b7d6e5d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Checking Data Types ----\n",
    "#\n",
    "# - all features must be considered int64\n",
    "#\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b51cdd-c25d-4910-9a96-38bf875a0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Checking for Missing Values ----\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebbfc1-aa5d-4116-8e5d-a8ed41708340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Checking out for Duplicated Values ----\n",
    "#\n",
    "# - since 'Interval' features measures the gap between the current prime number and the previous one,\n",
    "# there's no problem in this feature having duplicated values\n",
    "#\n",
    "# - however, we don't wanna consider the same number (Feature 'Num') twice, so we will be checking\n",
    "# for ANY duplicated values in it\n",
    "#\n",
    "#    \\ False: there no duplicated values\n",
    "#    \\ True: there are duplicated values\n",
    "#\n",
    "are_there_duplicated_values = df.duplicated('Num').any()\n",
    "duplicated_values = df[df.duplicated('Num')]\n",
    "\n",
    "print(f'Are there duplicated values? {are_there_duplicated_values}')\n",
    "duplicated_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32effa-de15-40e4-a2ea-69efc9edbb8d",
   "metadata": {},
   "source": [
    "Nice!! It looks like our dataset is quite perfect formatted, without containing missing and duplicated values! Thank you *Michael M. Ross*! Now, let's head straight to the funny part: `Statistic Analysis`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0ee07-6471-463f-b9d3-8288c67c93fa",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id=\"statistic-analysis\">üîç Statistic Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f9477-becc-494b-af45-48e1141f0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Distribution Plots ----\n",
    "#\n",
    "# - Frequence Distribution of Each Feature\n",
    "#\n",
    "df.hist(bins=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe22123-aac6-44fe-b074-9696433d5bc0",
   "metadata": {},
   "source": [
    "> **Num Distribution** - right-skewed distribution, indicating that the amount of number primes into a interval decreases as higher are the interval boundaries. So we can say that we can find more prime numbers in a range from `1 to 0.5 million` than a range from `0.5 million to 1.0 million`;\n",
    "\n",
    "> **Interval Distribution** - right-skewed distribution, indicating that the gap between each prime number is not too large. So we can say that the number primes are closer to each other rather than being distant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4b5f-c893-49f5-8016-68402cc01d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Box Plot ----\n",
    "#\n",
    "# - Spreadness of Each Feature\n",
    "#\n",
    "df.boxplot('Num');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1cf836-48e4-486c-9791-de45cf3df537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot('Interval');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c449b4-7b4b-429d-b07a-dfc2ea71f393",
   "metadata": {},
   "source": [
    "> **Num Box-Plot** - there are no visual outliers, the range goes from 0 to 1.5 million and 50% of prime numbers falls into 0.4 and 1.1 million range;\n",
    "\n",
    "> **Interval Box-Plot** - there are several visual outliers being all values greater than 40, the range goes from 0 to 150 and 50% of intervals between prime numbers falls into 5 and 20 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc85f45-7a1d-4f2e-843f-bcb63330744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Statistic Overview ----\n",
    "def describe(df, stats):\n",
    "    \"\"\"\n",
    "    Add statistics metrics to a DataFrame. Mean Absolute Deviation (MAD) is always added.\n",
    "    \n",
    "    Since the function 'mad' will become deprecated in the next pandas versions, MAD must be \n",
    "calculated manually.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---- Calculating the Common Describe ----\n",
    "    common_describe = df.describe()\n",
    "    \n",
    "    # ---- Calculating Mean Absolute Deviation (MAD) ----\n",
    "    mad_describe = lambda df: pd.DataFrame((df - df.mean()).abs().mean()).T.set_index(pd.Index(['mad']))\n",
    "    mad_df = mad_describe(df)\n",
    "    \n",
    "    # ---- Calculating Other Statistics ----\n",
    "    stats_df = df.reindex(df.columns, axis=1).agg(stats)\n",
    "    \n",
    "    # ---- Concatenating the Results ----\n",
    "    return pd.concat([common_describe, mad_df, stats_df]).reindex(['count', 'mean', 'sem', 'var', 'std',  'median', 'mad', 'min',  '25%', '50%', '75%',  'max', 'skew',  'kurt'])\n",
    "\n",
    "statistical_df = describe(df, ['median', 'var', 'skew', 'kurt', 'sem'])\n",
    "statistical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11da9aa-a51a-4b0c-9711-a16c1ab93867",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_frequency = pd.crosstab(\n",
    "    index=df['Interval']\n",
    "    , columns=['frequency']\n",
    ")\n",
    "\n",
    "interval_frequency.sort_values(by='frequency', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1994a-1be9-4634-96d0-de95ad576cbd",
   "metadata": {},
   "source": [
    "Here are what we can conclude with these info:\n",
    "\n",
    "> **Interval Mode** - the more common gap between prime numbers is `6 numbers`, indicating that many prime numbers are often found near multiples of 6. This info has been already discovered, is called `Euler's 6n + 1 Theorem` and it's equation is given as below:\n",
    "\n",
    "$$6n \\pm 1$$\n",
    "\n",
    "However, even though this equation working out for the majority of prime numbers, it has already been proved to don't work in all cases where it tells that non-primes are primes and primes are non-primes.\n",
    "\n",
    "----\n",
    "\n",
    "> **Num Spreadness - STD vs MAD** - the *Standard Deviation (std)* and the *Mean Absolute Deviation (MAD)* are given as `4.523191e+06` and `3.923997e+06` respectively, being `5.992194e+05` the difference between them. Considering that the numbers are infinite and that this dataset contains numbers in a range from 1 to 1.5 million, we can conclude that the datas are not so spread in order to have outliers.\n",
    "\n",
    "> **Num Mean VS Median** - since the difference between the mean and median is not so large to this dataset, `1.041720e+05`, we can use `Mean and STD` rather than `Median and MAD`;\n",
    "\n",
    "> **Num Skew** - being equals to `5.867023e-02 (0.05..)` and distant from 1, we can conclude that the distribution is not so skewed. We can realize that looking at its previous distribution plot, where we can notice a skewed curve in the beginning and almost a straight line in the middle to the end;\n",
    "\n",
    "> **Num Kurt** - being equals to `-1.213029e+00 (-1.21...)` and smaller than 0, we can conclude that the datas are more centered rather than spread over the distribution tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16259e75-a334-4720-b458-4ed054c0725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Correlation ----\n",
    "correlations = df.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6115a25-fa82-4d92-8000-f341dc83642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    correlations\n",
    "    , cmap='crest'\n",
    "    , annot=True\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a91ebd-32bf-4a1b-9fc7-8d32aee04528",
   "metadata": {},
   "source": [
    "Let's examine the correlation between Num and Interval Features. The correlation coefficient between these features is `0.071`. Since this value is significantly distant from 1, we can conclude that both features are not strongly correlated.\n",
    "\n",
    "----\n",
    "\n",
    "To wrap up the Statistical Analysis step, we can take three tests. The first one is a `One Sample T-Test` to check out if the dataset's mean differs from the population mean and conclude if all previous statistics are applied to the population or just to the sample. However, since we can't have the population's mean, we will discard this test and continue the analysis inferring that the previous statistics are applied to the dataset only.\n",
    "\n",
    "The second is a `Two Sample T-Test` in order to check out if the difference of means from Num and Interval Features means has happened randomly.\n",
    "\n",
    "The third one is the `Pearson's Correlation Test` to enforce our Hypothesis that Num and Interval Features are not strongly correlated. \n",
    "\n",
    "These last two tests are possible to be implemented here, so we will be taking them right now! For the tests, consider:\n",
    "\n",
    "> **Confidence Level** - `95% (0.95)`;\n",
    "\n",
    "> **Significance Level** - `5% (0.05)`;\n",
    "\n",
    "> **Target P-Value to Reject the Null Hypothesis** - `smaller than or equals 0.05`.\n",
    "\n",
    "First, let's see a briefly explanation about how these two tests work.\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Two Sample T-Test</b>\n",
    "\n",
    "`Two Sample T-Test` is used when you desire to check out whether two features means don't differ by chance, that is, there's an explanation and correlation behind it.\n",
    "\n",
    "```python\n",
    "t_test_value = r / sqrt((1 - r**2) / (n - 2))\n",
    "```\n",
    "\n",
    "$$\n",
    "{\\text{T-Test Value}} = \\frac{r}{\\sqrt{\\frac{1 - r^2}{n - 2}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "> **r** - correlation coefficient between the two features;\n",
    "\n",
    "> **n** - sample's size.\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Pearson's Correlation Test</b>\n",
    "\n",
    "`Pearson's Correlation Test` is used when you desire to check out whether two variables have correlation or not. This test is done following the steps below:\n",
    "\n",
    "- Step 1: calculate the `covariance` between the features.\n",
    "\n",
    "```python\n",
    "cov(x, y) = sum((x - mean(x)) * (y - mean(y))) / (n - 1)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\text{cov}(x, y) = \\frac{\\sum((x - \\text{mean}(x)) \\cdot (y - \\text{mean}(y)))}{(n - 1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "> **x and y** - features;\n",
    "\n",
    "> **n** - sample's size.\n",
    "\n",
    "- Step 2: calculate `Pearson's Correlation` using the covariance.\n",
    "\n",
    "```python\n",
    "p(x, y) = cov(x, y) / (std(x) * std(y))\n",
    "```\n",
    "\n",
    "$$\n",
    "p(x, y) = \\frac{\\text{cov}(x, y)}{\\text{std}(x) \\cdot \\text{std}(y)}\n",
    "$$\n",
    "\n",
    "- Step 3: accordingly to Pearson's Correlation value, we can assume:\n",
    "\n",
    "> **p(x, y) = -1** - there is a `negative correlation` between the features;\n",
    "\n",
    "> **p(x, y) = 0** - there is `no correlation`;\n",
    "\n",
    "> **p(x, y) = 1** - there is a `positive correlation` between the features.\n",
    "\n",
    "----\n",
    "\n",
    "For these tests, we will use functions from `scipy.stats` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335e2c0-a919-4538-a423-b608c5561b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Two Sample T-Test ----\n",
    "#\n",
    "# - Null Hypothesis (H0) > 'Num' and 'Interval' Features Means differs by chance;\n",
    "#\n",
    "# - Alternative Hypothesis (H1) > 'Num' and 'Interval' Features Means does not differ randomly, so there is an explanation behind it,\n",
    "# such as Riemann Hypothesis and Euler's 6n + 1 Theorem.\n",
    "#\n",
    "ttest_t_statistic, ttest_p_value = stats.ttest_ind(\n",
    "    a=df['Num']            # feature 1\n",
    "    , b=df['Interval']     # feature 2\n",
    "    , equal_var=False      # assumes that features have the same variance (True, False)\n",
    "    , random_state=SEED    # reproducability\n",
    ")\n",
    "\n",
    "print(f'Difference Between the Two Features Means (T-Satatistic): {ttest_t_statistic}')\n",
    "print(f'P-Value: {ttest_p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e7ce5-d88b-4865-bef0-692222f6b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Pearson's Correlation Test ----\n",
    "#\n",
    "# - Null Hypothesis (H0) > 'Num' and 'Interval' Features are not correlated;\n",
    "#\n",
    "# - Alternative Hypothesis (H1) > 'Num' and 'Interval' Features are correlated.\n",
    "#\n",
    "pearson_t_statistic, pearson_p_value = stats.pearsonr(df['Num'], df['Interval'])\n",
    "\n",
    "print(f'Pearson Correlation (T-Statistic): {pearson_t_statistic}')\n",
    "print(f'P-Value: {pearson_p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133f4a8-03f8-4780-8b0d-76bdbe2ebdbb",
   "metadata": {},
   "source": [
    "> **Two Sample T-Test** - since we got a p-value equals `0.0`, we can **reject** the Null Hypothesis and tell that the features means don't differ by chance, having an explanation behind it, such as *Riemann Hypothesis* and *Euler's 6n+1 Theorem*;\n",
    "\n",
    "> **Pearson's Correlation Test** - since we got a p-value equals `0.0`, we can **reject** the Null Hypothesis and tell that both features are correlated, however, as far as t-statistic value is too small (*0.0712*), the linear correlation is not strong enough.\n",
    "\n",
    "----\n",
    "\n",
    "There are a bunch of other Statistic Analysis techniques we can apply with this dataset that we can cover in other ooportunity. Just for further analysis, let's generate a `Pandas Report` of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975708f-adee-4929-930c-e6fb75d04d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Report Generation ----\n",
    "df.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e5e90-5c8e-4332-81e6-fd9fe24c0ca7",
   "metadata": {},
   "source": [
    "<h2 id='data-transformations'>üîç Data Transformations</h2>\n",
    "\n",
    "For this task, we will achieve our first goal: create two files, `natural_numbers.csv` and `natural_numbers.parquet`, containing all numbers into the range from 1 to the first million, but we will also apply some feature engineering.\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Target Leakage</b>\n",
    "\n",
    "`Target Leakage` is a type of `Data Leakage` that tells that a feature got its value defined **after** knowing the target feature value. For instance, consider the Interval feature and IsPrime one as being our target. Do you agree that `Interval` got its values defined after knowing `IsPrime` values? Because, to tell the gap between two prime numbers, we gotta know if the numbers are primes or not before, right?\n",
    "\n",
    "With this in mind, we can tell that all features that falls into Target Leakage will make our models learn noises rather than signals and generate bad results.\n",
    "\n",
    "For this reason, we will **drop Interval Feature**!!\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Adding More Observations</b>\n",
    "\n",
    "Currently, the `Num` Feature contains just **prime numbers** from 1 to 1.5 million and if our model be trained with this dataset, it'll just receive prime numbers to memorize and not to learn patterns. So we will complement this feature adding all non-prime numbers within this range.\n",
    "\n",
    "Also, all these numbers will have the target feature `IsPrime` set as False.\n",
    "\n",
    "----\n",
    "\n",
    "<b>- Target Feature</b>\n",
    "\n",
    "Also, we have to add a new feature called `IsPrime` informing whether the observation is a prime number or not.\n",
    "\n",
    "----\n",
    "\n",
    "So, hands-on!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037de211-0eb2-4102-815f-14e3978faac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Creating New Dataset ----\n",
    "#\n",
    "# \\ dropping 'Interval' feature\n",
    "# \\ adding 'IsPrime' feature\n",
    "#\n",
    "transformed_df = df.copy()\n",
    "transformed_df.drop('Interval', axis=1, inplace=True)\n",
    "transformed_df['IsPrime'] = transformed_df['Num'].map(lambda number: True)\n",
    "\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8522e3b-faa5-4b0b-9071-9ea21a9668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Creating New Dataset ----\n",
    "#\n",
    "# \\ complementing 'Num' and 'IsPrime' features\n",
    "#\n",
    "existing_numbers = set(df['Num'])\n",
    "all_numbers = set(range(df['Num'].min(), df['Num'].max() + 1))\n",
    "remaining_numbers_df = pd.DataFrame({ 'Num': list(all_numbers - existing_numbers) })\n",
    "\n",
    "transformed_df = pd.concat([transformed_df, remaining_numbers_df], ignore_index=True)\n",
    "transformed_df.sort_values(by='Num', ascending=True, inplace=True, ignore_index=True)\n",
    "transformed_df['IsPrime'].fillna(False, inplace=True)\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22801598-b94e-48a7-9159-68946b2a97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Saving Dataset into CSV and Parquete Files ----\n",
    "transformed_df.to_csv(NATURAL_NUMBERS_FILE_PATH_CSV, index=False)\n",
    "transformed_df.to_parquet(NATURAL_NUMBERS_FILE_PATH_PARQUET, engine='pyarrow', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a57c0c-8ab9-4c0a-95b1-a7b8348b00fa",
   "metadata": {},
   "source": [
    "> **‚úîÔ∏è Goal 1** - create natural_numbers.csv and natural_numbers.parquet files to store all numbers from 1 to the first million prime;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818e01e-06a2-4e45-bd68-1e426eeff1a1",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id='riemann-hypothesis-implementation'>üîç Riemann Hypothesis Implementation</h2>\n",
    "\n",
    "For Riemann Hypothesis Implementation, we will use the `zeta` function from `scipy.special` submodule, assigning 1 to the `q parameter`.\n",
    "\n",
    "For further information about this function, check out its documentation: [scipy.special.zeta](https://docs.scipy.org/doc/scipy-1.7.0/reference/reference/generated/scipy.special.zeta.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fcce2-020e-48ff-9846-61662e43c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import zeta\n",
    "\n",
    "number = 5\n",
    "zeta(x=number, q=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02dfab-b0de-435e-9a24-4c740e21b9be",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "He he, so the zeta funtion for 5 returns approximately `1.037` and you know what does it mean about 5 being prime or not? I'll give you a spoiler, nothing!!\n",
    "\n",
    "The zeta function in scipy just calculates, literally, the Riemann's Zeta Value, nothing more and or nothing less - if you have watched the Riemann Hypothesis video that I mentioned in the beggining of this notebook, you'll know what I'm talking about.\n",
    "\n",
    "But don't worry, there's a *hack* function from `sympy` library that give us a hand for this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79761707-a22a-44a0-8b82-6175d5835f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    number = int(input('Type any number:'))\n",
    "    print(f'Result: {sympy.isprime(number)}')\n",
    "except:\n",
    "    print('You did not typed a number!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47da4fd-c58f-4922-a55c-c3463ce06312",
   "metadata": {},
   "source": [
    "> **‚úîÔ∏è Goal 2** - implement Riemann Hypothesis without Machine Learning Algorithm;\n",
    "\n",
    "But since our goal is not use a built-in function from libraries, let's create our own Machine and Deep Learning Model to classify the numbers!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7620a984-8a4b-4610-99f9-31e4465cb549",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id='preparing-the-dataset'>üîç Preparing the Dataset</h2>\n",
    "\n",
    "For this task, let's do four things:\n",
    "\n",
    "- read the full dataset;\n",
    "\n",
    "- encode `IsPrime` feature;\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Current Value</th>\n",
    "        <th>Encoded Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>True</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>False</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- set our feature (*Num*) and target(*IsPrime*);\n",
    "\n",
    "- check out whether dataset is balanced and imbalanced;\n",
    "\n",
    "- split it up into train (*80%*) and validation (*20%*) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a5ac47-c5c2-458a-9139-f1ada5935931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Observations: 15485856\n",
      "# of Features: 2 (Index(['Num', 'IsPrime'], dtype='object'))\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num</th>\n",
       "      <th>IsPrime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num  IsPrime\n",
       "0    2     True\n",
       "1    3     True\n",
       "2    4    False\n",
       "3    5     True\n",
       "4    6    False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Reading Full Dataset ----\n",
    "full_df = pd.read_csv(NATURAL_NUMBERS_FILE_PATH_CSV)\n",
    "\n",
    "print(f'# of Observations: {full_df.shape[0]}')\n",
    "print(f'# of Features: {full_df.shape[1]} ({full_df.columns})')\n",
    "print('----')\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654f302d-1c64-4820-a67d-b34100c1ef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num</th>\n",
       "      <th>IsPrime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num  IsPrime\n",
       "0    2        1\n",
       "1    3        1\n",
       "2    4        0\n",
       "3    5        1\n",
       "4    6        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Encoding 'IsPrime' Feature ----\n",
    "full_df['IsPrime'] = full_df['IsPrime'].astype(int)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835db78f-817d-4d92-927c-cbcddb635876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Setting Up Feature and Target ----\n",
    "X = full_df.copy()\n",
    "y = X.pop('IsPrime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97c5d29-14a3-4f10-9a78-08cd4b37a8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsPrime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.064575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0       count\n",
       "IsPrime          \n",
       "0        0.935425\n",
       "1        0.064575"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Checking out if Dataset is Imbalanced ----\n",
    "frequency = pd.crosstab(index=y, columns='count')\n",
    "frequency / frequency.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30576c04-7758-4da5-8e3b-e0ef1bc0b963",
   "metadata": {},
   "source": [
    "The table above shows that approximatelly `94%` of numbers are not primes and `6%` are, kind of a huge gap, isn't it?\n",
    "\n",
    "Since there's this huge gap, we call that the dataset is `imbalanced for non-prime numbers` and, consequently, we gotta stratify the training and validation target datasets and use `Balanced Accuracy` rather than `Accuracy` for our Machine Learning Models!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53538efc-e1af-4628-a73f-55255ab67f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = set(X.index)\n",
    "train_indexes = set(X.sample(frac=0.7, random_state=SEED).index)\n",
    "valid_indexes = full_index - train_indexes\n",
    "\n",
    "X_train = X.iloc[list(train_indexes)]\n",
    "X_valid = X.iloc[list(valid_indexes)]\n",
    "\n",
    "y_train = y.iloc[list(train_indexes)]\n",
    "y_valid = y.iloc[list(valid_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f44fce-735f-4091-bf5d-e0ceeb6fed1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03ffde-da5d-47e9-9781-2ef4e895b67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f82c31-1d22-44a8-8cde-3850b9136953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Splitting up Dataset into Train and Validation ----\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, random_state=SEED\n",
    "    , train_size=0.70\n",
    "    , test_size=0.30\n",
    "    , shuffle=True\n",
    "    , stratify=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2754fd-2b77-4536-8931-49cf6c95715b",
   "metadata": {},
   "source": [
    "From *Stack Overflow* best answer: [Parameter \"stratify\" from method \"train_test_split\" (scikit Learn)](https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn).\n",
    "\n",
    "> This `stratify` parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n",
    "\n",
    "> For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457eb357-c433-4cac-bc04-221e377b2ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsPrime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.93541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      count\n",
       "IsPrime         \n",
       "0        0.93541\n",
       "1        0.06459"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Training Outcomes Frequencies ----\n",
    "y_train_frequency = pd.crosstab(index=y_train, columns='count')\n",
    "y_train_frequency / y_train_frequency.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36256ca-a321-44ae-a8eb-8a9318b3b3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsPrime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.93541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      count\n",
       "IsPrime         \n",
       "0        0.93541\n",
       "1        0.06459"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Validation Outcomes Frequencies ----\n",
    "y_valid_frequency = pd.crosstab(index=y_train, columns='count')\n",
    "y_valid_frequency / y_valid_frequency.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6156f1b-3f8b-4b0d-874f-c2e4806cccff",
   "metadata": {},
   "source": [
    "Okay, now that we confirmed that both training and validation outcomes are stratified, let's head straight to the Machine Learning Models!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d245f8b-2ff0-4d21-9f20-d65b1b6b4ce0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<h2 id='machine-learning-models'>üîç Machine Learning Models</h2>\n",
    "\n",
    "For this topic, let's create two models!\n",
    "\n",
    "```\n",
    "- Logistic Classifier;\n",
    "- Support Vector Machine.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6a670f-fa5b-4bf2-a970-7d8aadcf7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231859f4-f189-4d44-86a0-d7ef1b5c7408",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<b>- Logistic Classifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "802715a1-64d3-41ba-a107-bdfea14a91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9354611099977894"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Logistic Classifier ----\n",
    "logistic_classifier = LogisticRegression(\n",
    "    n_jobs=4\n",
    "    , random_state=SEED\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "predictions = logistic_classifier.predict(X_valid)\n",
    "\n",
    "balanced_accuracy_score(predictions, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e3f4540-ff8e-4e0e-a3bf-1cd8409dbb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4645757}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfca52-3ec7-4670-b36e-6e8d3c65f782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
